{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHh+zSEGo4MEi645fAZmXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chayan141/Data_Science_Certifications/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning involves combining predictions from multiple machine learning models to produce a single, more accurate prediction than any individual model."
      ],
      "metadata": {
        "id": "L9LQWNGrQ3Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging: Train the multiple instances of the same model on different random subsets of the training data and combine predictions by averaging (Regression) or majority voting (Classification)."
      ],
      "metadata": {
        "id": "krtmp3_8Q87P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest, which uses bagging with decision trees and adds feature randomness for diversity."
      ],
      "metadata": {
        "id": "0Klz9AJ_RWtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting: Sequentially train models where each model focuses on correcting the errors of the previous ones by assigning higher weights to misclassified or poorly predicted instances.\n",
        "\n",
        "Adaboost, Gradient Boosting, XGBoost, LightGBM, CatBoost. Reduces bias and variance."
      ],
      "metadata": {
        "id": "qSSc5Eh2Rklq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voting: Combines predictions from multiple different models (e.g. logistic regression, SVM, decision trees) using majority voting (classification) or averaging (regression).\n",
        "\n",
        "Hard Voting: Selects class with the most vote.\n",
        "\n",
        "Soft Voting: Averages predicted probabilities for a weighted vote."
      ],
      "metadata": {
        "id": "C1DUHMKdSKvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking: Train multiple base models (level 0 models), then use their predictions as input features for a higher level model (meta-learner or level one model)."
      ],
      "metadata": {
        "id": "IHdevW1ZS6Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blending: Train the base models on the training set the use a holdout validation set to generate predictions for the meta learner.\n",
        "\n",
        "Split the data between training set and holdout set. Train the level -0 models or base models with training set. Generate predictions of a holdout set then these predictions becomes input to meta learner."
      ],
      "metadata": {
        "id": "IgdJPD4qT-Fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xML4NfrgQffs"
      },
      "outputs": [],
      "source": []
    }
  ]
}